{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1sQ9BGHqBmZ"
      },
      "source": [
        "# Assignment 3 #\n",
        "### Due: Tuesday, October 10th to be submitted via Canvas by 11:59 pm ###\n",
        "### Total points: **90** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y86kiHXJYHCy"
      },
      "source": [
        "Your homework should be written in a python notebook. If you prefer, you can work in groups of two. **Please note that only one student per team needs to submit the assignment on Canvas but make sure to include both students' names, UT EIDs and the homework group.**  \n",
        "\n",
        "For any question that requires a handwritten solution, you may upload scanned images of your solution in the notebook or attach them to the assignment . You may write your solution using markdown as well.\n",
        "\n",
        "Please make sure your code runs and the graphs and figures are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "1. Homework Group -\n",
        "2. Student Names -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_PNd32DnKcu"
      },
      "source": [
        "## Q1. (30 points) - Comparing MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YTDMq9RnKc0"
      },
      "source": [
        "In this problem, we will be comparing different MLP configurations on the California Housing dataset and the Diabetes dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5Ai294RnKc0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import fetch_california_housing, load_diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQkJF0MvnKc0"
      },
      "outputs": [],
      "source": [
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFT4Ji-tnKc0",
        "outputId": "ba72a1ee-ff6e-4596-e766-2dccdc3f0479"
      },
      "outputs": [],
      "source": [
        "# Load the Calinifornia Housing dataset and do train/val/test split\n",
        "california_housing = fetch_california_housing()\n",
        "housing_X, housing_y = california_housing['data'], california_housing['target']\n",
        "housing_X_train, housing_X_tmp, housing_y_train, housing_y_tmp = train_test_split(housing_X, housing_y, test_size=0.4, random_state=seed)\n",
        "housing_X_val, housing_X_test, housing_y_val, housing_y_test = train_test_split(housing_X_tmp, housing_y_tmp, test_size=0.5, random_state=seed)\n",
        "\n",
        "housing_scaler = StandardScaler()\n",
        "housing_X_train = housing_scaler.fit_transform(housing_X_train)\n",
        "housing_X_val = housing_scaler.transform(housing_X_val)\n",
        "housing_X_test = housing_scaler.transform(housing_X_test)\n",
        "print(california_housing['DESCR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TXe9gCsnKc0",
        "outputId": "5dc63917-302f-4712-cf2a-e31ef34ff596"
      },
      "outputs": [],
      "source": [
        "# Load the Diabetes dataset and do train/val/test split\n",
        "diabetes = load_diabetes()\n",
        "diabetes_X, diabetes_y = diabetes['data'], diabetes['target']\n",
        "\n",
        "diabetes_X_train, diabetes_X_tmp, diabetes_y_train, diabetes_y_tmp = train_test_split(diabetes_X, diabetes_y, test_size=0.4, random_state=seed)\n",
        "diabetes_X_val, diabetes_X_test, diabetes_y_val, diabetes_y_test = train_test_split(diabetes_X_tmp, diabetes_y_tmp, test_size=0.5, random_state=seed)\n",
        "\n",
        "diabetes_scaler = StandardScaler()\n",
        "diabetes_X_train = diabetes_scaler.fit_transform(diabetes_X_train)\n",
        "diabetes_X_val = diabetes_scaler.transform(diabetes_X_val)\n",
        "diabetes_X_test = diabetes_scaler.transform(diabetes_X_test)\n",
        "print(diabetes['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBtypTpYnKc0"
      },
      "source": [
        "**Part 1** **(10 pts)**. Write the training and evaluation functions of the MLP. Use the default parameter values of sklearn.neural_network.MLPRegressor except:\n",
        "\n",
        "*   **hidden_layer_size**: given by train_mlp parameter\n",
        "*   **learning_rate_init**: given by a list of search space\n",
        "*   **random_state**: given by train_mlp parameter\n",
        "*   **max_iter**: fix at 300\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vNvRJXBnKc0"
      },
      "outputs": [],
      "source": [
        "def train_mlp(hidden_layer_size, X_train, y_train, X_val, y_val, seed):\n",
        "    learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "    best_r2 = -np.inf # Determine the best model based on the R2 score on validation set # TO HIDE\n",
        "    for lr in learning_rate_list:\n",
        "\n",
        "        # Initialize MLPRegressor\n",
        "        ### START CODE ###\n",
        "        model = MLPRegressor(hidden_layer_sizes=hidden_layer_size, learning_rate_init=lr, random_state=seed, max_iter=300)\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Fit the MLPRegressor to training data\n",
        "        ### START CODE ###\n",
        "        model.fit(X_train, y_train)\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Predict and evaluate on train and validation data\n",
        "        mse_train, r2_train = eval_model(model, X_train, y_train)\n",
        "        mse_val, r2_val = eval_model(model, X_val, y_val)\n",
        "        print(f\"Learning rate: {lr} MSE train: {mse_train} R2 train: {r2_train} MSE val: {mse_val} R2 val: {r2_val}\")\n",
        "\n",
        "        # Record the best model according to R2 score on validation set\n",
        "        if r2_val > best_r2:\n",
        "            ### START CODE ###\n",
        "            best_model = deepcopy(model)\n",
        "            best_lr = lr\n",
        "            ### END CODE ###\n",
        "\n",
        "    return best_model, best_lr, best_r2\n",
        "\n",
        "def eval_model(model, X, y):\n",
        "    # Predict and evaluate\n",
        "    ### START CODE ###\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    ### END CODE ###\n",
        "    return mse, r2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vlQE-atnKc1"
      },
      "source": [
        "**Part 2** **(5 pts)**.\n",
        "Train two MLPs on the **housing dataset** with the following two different hidden layer size configurations and show their **MSE** and **R2 score** on the **test set**.\n",
        "*   (8)\n",
        "*   (64, 64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_1 = train_mlp((8), housing_X_train, housing_y_train, housing_X_val, housing_y_val, seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1_mse, model1_r2 = eval_model(model_1[0], housing_X_test, housing_y_test)\n",
        "print(\"mse: \" + str(model1_mse))\n",
        "print(\"r2: \" + str(model1_r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_2 = train_mlp((64, 64), housing_X_train, housing_y_train, housing_X_val, housing_y_val, seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2_mse, model2_r2 = eval_model(model_1[0], housing_X_test, housing_y_test)\n",
        "print(\"mse: \" + str(model2_mse))\n",
        "print(\"r2: \" + str(model2_r2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQX-c2vEnKc1"
      },
      "source": [
        "**Part 3** **(5 pts)**. Train a **linear regression model** on the **housing dataset** and show their MSE and R2 scores on the test set. How do the performances of the two MLPs and the linear regression model compare, and what do you think causes the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HMizilSnKc1"
      },
      "source": [
        "**Part 4** **(5 pts)**. Now, train two MLPs on the **diabetes dataset** with the following two different hidden layer size configurations and show their **MSE** and **R2 score** on the **test set**.\n",
        "*   (8)\n",
        "*   (64, 64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrBLzeqnKc1"
      },
      "source": [
        "**Part 5** **(5 pts)**. Train another linear regression model on the **diabetes dataset** and show its **MSE** and **R2 score** on the **test set**. How do the performances of two MLPs and the linear regression model compare? Is the performance order on the diabetes dataset the same as the one on the housing dataset? If not, what causes the difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pb9fxiUfPsv"
      },
      "source": [
        "#Q2. (20 points) Tensorflow Playground\n",
        "In this question, you will be playing with [Tensorflow Playground](https://playground.tensorflow.org).\n",
        "\n",
        "\n",
        "Select **Classification** as the Problem Type. Among the four datasets shown in DATA, please select the top left dataset.\n",
        "\n",
        "Use the following settings as the DEFAULT settings for all subquestions:\n",
        "\n",
        "\n",
        "*   Learning rate = 0.03\n",
        "*   Activation = Tanh\n",
        "*   Regularization = None\n",
        "*   Ratio of training to test data = 50%\n",
        "*   Noise = 0\n",
        "*   Batch Size = 30\n",
        "*   input as  ùëã1  with  ùëã2\n",
        "*   One hidden layer with 4 neurons\n",
        "\n",
        "a) **(4 pts)** Use the DEFAULT setting and run two experiments -\n",
        "\n",
        "1.   Using Tanh as the activation function\n",
        "2.   Using the Linear activation function.\n",
        "\n",
        "Report the train, test losses for both at the end of 1000 epochs. What qualitative difference do you observe in the decision boundaries obtained? What do you think is the reason for this?\n",
        "\n",
        "We will now study the effect of certain variations in the network structure or training process, keeping all other aspects the same as in the DEFAULT setting specified above, with Tanh as the activation.\n",
        "\n",
        "b) **(4 pts)** Effect of number of hidden units: Keep other settings the same as in DEFAULT.\n",
        "\n",
        "\n",
        "1.   Report the training loss and test loss at the end of 1000 epochs using 2 neurons and 8 neurons in the hidden layer.\n",
        "2.   What do you observe in terms of the decision boundary obtained as the number of neurons increases? What do you think is the reason for this?\n",
        "\n",
        "c) **(4 pts)** Effect of Learning rate and number of epochs: Keep other settings the same as in DEFAULT.\n",
        "\n",
        "1.   For learning rate 10, 1, 0.1, 0.01 and 0.0001, report the train, test losses at the end of 100 epochs, 500 epochs and 1000 epochs respectively.\n",
        "2.   What do you observe from the change of loss vs learning rate, and the change of loss vs epoch numbers? Also report your observations on the training and test loss curve (observe if you see noise for certain learning rates and reason why this is happening).\n",
        "\n",
        "\n",
        "\n",
        "d) **(4 pts)** Effect of the number of layers:\n",
        "\n",
        "1.   Change your activation to ReLU and use a single hidden layer with 4 neurons and then add another hidden layer with 3 neurons and train both your models for 1000 epochs.\n",
        "2.   Comment on your final models and decision boundaries and observe your training and test loss curves as well.\n",
        "\n",
        "\n",
        "e) **(4 pts)** Use the DEFAULT setting. Play around with any hyperparameters, network architectures or input features (such as  sin(ùëã1),ùëã21  etc.), and report the best train and test loss you obtain (test loss should be no greater than 0.06). Attach the screenshot showing your full network, output and the parameters. Briefly justify your results, and comment on what helps/what doesn't help with lowering the loss, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djNl4dMSwWC7"
      },
      "source": [
        "# Q3. (10 points) - Principal Component Analysis\n",
        "\n",
        "Consider a set of data points $\\{x_1, x_2, ... x_N\\}$ where each $x_i \\in \\mathbb{R}^d$, given to you after centering, i.e., $\\frac{1}{N}\\sum_{i=1}^{N}x_i = 0$.\n",
        "\n",
        "Now suppose you want to project this data on a single unit vector given by $u$ by learning an appropriate $u$. Show that, for learning $u$, minimizing the residual of the projections computed by the squared error between the projected data and the original data is equivalent to maximizing the variance of the projections. Hint : the projection of an $x$ on a unit vector $u$ is given by $(x^{T}u)u$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7czWThSxqLHZ"
      },
      "source": [
        "# Q4. (20 points) - Principal Component Analysis\n",
        "\n",
        "In this problem we will be applying PCA and T-SNE on the Superconductivity Dataset. More details on the dataset is present [here](https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data#). The goal here is to predict the critical temperature of a superconductor based on the features extracted.\n",
        "\n",
        "First use Principal Component Analysis (PCA) to solve this problem.  \n",
        "\n",
        "* **Part 1. (5 points)** Perform the following steps to prepare the dataset:\n",
        "    * Load the dataset from the \"Q4data.csv\" file provided as a dataframe df.\n",
        "\n",
        "    * Select the **'critical_temp'** column as the target column and the rest of the columns from the dataframe df as X.\n",
        "\n",
        "    * Split the dataset into train and test set with 35% data in test set and random_state = 42\n",
        "\n",
        "    * Perform [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) on the dataset. Remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y.\n",
        "    \n",
        "    `Note: X should have 81 features.`\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "2qKTTk2I6Q2i",
        "outputId": "d5cd566c-117b-43e5-9dda-431157e6d653"
      },
      "outputs": [],
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on ‚ÄúChoose Files‚Äù then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_YcaHt-tnLC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "import os, sys, re\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv(\"Q4data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ_vteTAtqj-"
      },
      "outputs": [],
      "source": [
        "y = df[\"critical_temp\"]\n",
        "X = df.drop(columns=[\"critical_temp\"])\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "scalar = StandardScaler()\n",
        "\n",
        "### START CODE ###\n",
        "### Scale the dataset\n",
        "### END CODE ###\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhjMtKvt-c-"
      },
      "source": [
        "* **Part 2 (5 points)** Use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and reduce the dimension of X_train to the following number of components: `[3,20,40,60,81]`. For each of the five datasets, print the cumulative variance explained by the principal components`N = [3,20,40,60,81]`.(i.e. what percentage of variance in the original dataset is explained if we transform the dataset to have 3,20,40,60 and 81 principal components respectively).\n",
        "\n",
        "  `Note : PCA should be fit on X_train and the components thus learnt should be later used to transform X_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_RBsvNGuB-f",
        "outputId": "c4cb8a11-45dd-470d-de3a-1a34900b152a"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "nums = [3,20,40,60,81]\n",
        "res = []\n",
        "for num in nums:\n",
        "    ### START CODE ###\n",
        "    ## Fit PCA\n",
        "    ### END CODE ###\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Transform Data\n",
        "    ### END CODE ###\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Compute explained variance\n",
        "    ### END CODE ###\n",
        "\n",
        "\n",
        "    print(\"Cumulative variance explained by {} components is {}\".format(num,var[num-1])) #cumulative sum of variance explained with [n] features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msXHrOduuD7U"
      },
      "source": [
        "* **Part 3. (5 points)** Plot the cumulative variance explained by the principal components using the training data. The plot should display the number of components on X-axis and the cumulative explained variance on the y-axis. What do you understand from the plot obtained?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "K-eBOo9dt-6v",
        "outputId": "88b8485f-8151-4671-9f67-2debb155c087"
      },
      "outputs": [],
      "source": [
        "### START CODE ###\n",
        "## Plot the explained variance vs number of components\n",
        "### END CODE ###\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqXkjgDjuIRn"
      },
      "source": [
        "* **Part 4. (5 points)** For each of the reduced dataset, obtained in part 2.2, fit a linear regression model on the train data and show how adjusted $R^2$ varies as a function of # of components.(There will be a total of 5 ${R^2}$ score).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvUD1cYttsnd",
        "outputId": "a3ec6a3e-c5b4-4501-c3a6-3fcb571aee61"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "nums = [3,20,40,60,81]\n",
        "res = []\n",
        "for num in nums:\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Fit PCA components\n",
        "    ### END CODE ###\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Transform train and test data\n",
        "    ### END CODE ###\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Compute explained variance\n",
        "    ### END CODE ###\n",
        "\n",
        "    ### START CODE ###\n",
        "    ## Fit LR and compute R-square and adjusted R-squared\n",
        "    ### END CODE ###\n",
        "    adjusted_r_squared = 1 - (1-r_squared)*(len(Y_test)-1)/(len(Y_test)-X_test_new.shape[1]-1)\n",
        "    print(\"Adjusted R^2\",adjusted_r_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79S5IsAIIZ9A"
      },
      "source": [
        "# Q5. (10 points) PCA vs T-SNE\n",
        "* **Part 1.** **(3 points)** Now apply T-SNE to the dataset given above in Q4. You are required to carry out the following tasks:\n",
        "\n",
        "\n",
        "\n",
        "1.   Initialize a t-SNE model with number of dimensions = 3, perplexity = 300, number of iterations = 300 and random state = 42\n",
        "2.   Apply the t-SNE model to the training dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-PgOjrK7GDw"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "### START CODE ###\n",
        "## Initialize t-SNE model\n",
        "### END CODE ###\n",
        "\n",
        "### START CODE ###\n",
        "## Fit and transform the data\n",
        "### END CODE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WZHiIcLB1f"
      },
      "source": [
        "* **Part 2.** (3 points) For this part use a small subset of 500 samples of the training dataset and plot the first three t-SNE components similar to the PCA implementation above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tB-kPi1G1aA"
      },
      "source": [
        "* **Part 3. (4 points)** Now we will plot the PCA and t-SNE projections of the data and compare the plots side-by-side to see the difference in scatters created by the two methods. You can use first 1000 data points for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "-2-xyBKnIARW",
        "outputId": "1fa49910-4c5d-49e9-d541-189a95a06151"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))  # Adjust the figure size as needed\n",
        "\n",
        "# First subplot (left)\n",
        "\n",
        "### START CODE ###\n",
        "### Obtain components from PCA and plot\n",
        "### END CODE ###\n",
        "\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, select the first subplot\n",
        "plt.title('PCA')\n",
        "\n",
        "# Second subplot (right)\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, select the second subplot\n",
        "\n",
        "### START CODE ###\n",
        "### scatter plot for t-SNE\n",
        "### END CODE ###\n",
        "\n",
        "plt.title('T-SNE')\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kificzy3KBbf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
