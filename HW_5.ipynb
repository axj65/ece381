{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0gfAbD1BRax"
      },
      "source": [
        "# Assignment 5 #\n",
        "### Due: Friday, November 17th to be submitted via Canvas by 11:59 pm ###\n",
        "### Total points: **65** ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omOSpxMmBTQE"
      },
      "source": [
        "# Q1: Support Vector Machines (10 points)\n",
        "\n",
        "In this question, we will explore support vector machines for the Spam Base dataset from the UCI repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0GbdHYxjLiok"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26ixmQ9qT6z3"
      },
      "outputs": [],
      "source": [
        " #!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlQyfxg0YuB0"
      },
      "outputs": [],
      "source": [
        "# from ucimlrepo import fetch_ucirepo\n",
        "# # fetch dataset\n",
        "# spambase = fetch_ucirepo(id=94)\n",
        "\n",
        "# # data (as pandas dataframes)\n",
        "# X = spambase.data.features\n",
        "# y = spambase.data.targets\n",
        "# print(\"Abstract:\", spambase.metadata['abstract'])\n",
        "# print(\"Number of instances:\", spambase.metadata['num_instances'])\n",
        "# print(\"Number of features:\", spambase.metadata['num_features'])\n",
        "# print(spambase.variables['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBBCbGIVnj5c"
      },
      "outputs": [],
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "#from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U78Fzdt1niL0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/spambase/spambase.data\", header=None)\n",
        "y = df[57]\n",
        "X = df.drop(columns=[57])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lmKGx3T8ZXci"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0     1     2    3     4     5     6     7     8     9   ...   47    48  \\\n",
              "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.0  0.00   \n",
              "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.0  0.00   \n",
              "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.0  0.01   \n",
              "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0  0.00   \n",
              "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.0  0.00   \n",
              "\n",
              "      49   50     51     52     53     54   55    56  \n",
              "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278  \n",
              "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028  \n",
              "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259  \n",
              "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191  \n",
              "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191  \n",
              "\n",
              "[5 rows x 57 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tukYZVbDZsG-"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "y = y.to_numpy().squeeze()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jdODxjI41xm"
      },
      "source": [
        "a. (5 points) Implement the following function to train SVMs with a specified kernel type, hyper-parameter search space, and random state on the Spam Base dataset. Do hyper-parameter search over $C$ using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), setting the number of folds to 5. After finding the best C, please use it to train the final model and return both the final model and the best C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "K90sxpVK7mED"
      },
      "outputs": [],
      "source": [
        "def search_best_svm(kernel, C_search_space, random_state):\n",
        "    best_score = -np.inf\n",
        "    for C in C_search_space:\n",
        "\n",
        "        # Initialize an SVM classifier with the specified kernel type, C value, and random state\n",
        "        ### START CODE ###\n",
        "        clf = SVC(kernel=kernel, C=C, random_state=random_state)\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Evaluate accuracy scores using 5-fold cross-validation scores\n",
        "        ### START CODE ###\n",
        "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Compute the average score and compare with the current best score to update the best C\n",
        "        ### START CODE ###\n",
        "        score = np.mean(scores)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_C = C  \n",
        "        ### END CODE ###\n",
        "        print(f\"C: {C} Avg Cross Val Score: {np.round(score, 4)}\")\n",
        "\n",
        "    print(f\"Best C: {best_C}\")\n",
        "\n",
        "    # Initialize the model using the specified kernel type, best C, and random state;\n",
        "    # and then fit the model using training set\n",
        "    ### START CODE ###\n",
        "    model = SVC(kernel=kernel, C=best_C, random_state=random_state)\n",
        "    model.fit(X_train, y_train)\n",
        "    ### END CODE ###\n",
        "    return model, best_C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ_MU9Fl6EIn"
      },
      "source": [
        "b. (3 points) Run the function you implemented above to train SVMs with the search space of $C$ being [$0.1, 1, 10, 100$], random state set to 42, with the following three popular kernels: (i) linear (ii) polynomial (iii) RBF (Gaussian). Evaluate your final models on the test set and report their accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "search_space = [0.1, 1, 10, 100]\n",
        "random_state = 42\n",
        "models = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C: 0.1 Avg Cross Val Score: 0.9214\n",
            "C: 1 Avg Cross Val Score: 0.9264\n",
            "C: 10 Avg Cross Val Score: 0.9283\n",
            "C: 100 Avg Cross Val Score: 0.9268\n",
            "Best C: 10\n",
            "Kernel: linear, Best C: 10, Accuracy: 0.9282998370450842\n",
            "C: 0.1 Avg Cross Val Score: 0.6895\n",
            "C: 1 Avg Cross Val Score: 0.7594\n",
            "C: 10 Avg Cross Val Score: 0.8428\n",
            "C: 100 Avg Cross Val Score: 0.8957\n",
            "Best C: 100\n",
            "Kernel: poly, Best C: 100, Accuracy: 0.9125475285171103\n",
            "C: 0.1 Avg Cross Val Score: 0.896\n",
            "C: 1 Avg Cross Val Score: 0.9243\n",
            "C: 10 Avg Cross Val Score: 0.9243\n",
            "C: 100 Avg Cross Val Score: 0.9123\n",
            "Best C: 10\n",
            "Kernel: rbf, Best C: 10, Accuracy: 0.935361216730038\n"
          ]
        }
      ],
      "source": [
        "for kernel in ['linear', 'poly', 'rbf']:\n",
        "    # Train the model using the search_best_svm function\n",
        "    model, best_C = search_best_svm(kernel, search_space, random_state)\n",
        "\n",
        "    # Store the model in the dictionary\n",
        "    models[kernel] = model\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Kernel: {kernel}, Best C: {best_C}, Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCxwqenL6nZ4"
      },
      "source": [
        "c. (2 points) Train a logistic regression model using the training set. Compare its performance with that of the SVMs trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Accuracy: 0.9223248234655079\n"
          ]
        }
      ],
      "source": [
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "y_pred = logistic_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9ca1b5"
      },
      "source": [
        "# Question 2 : Ensemble Methods for Classification (25 pts)\n",
        "\n",
        "In this question, we will compare the performances of different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) classifiers.\n",
        "\n",
        "We will look at the [GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit) dataset for this question. The dataset is extremely large so for this question we will only consider a subset which has been provided along with the notebook for this assignment. The dataset has already been split into train and test sets.\n",
        "\n",
        "The task is to predict the probability that someone will experience financial distress in the next two years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "a93b913f",
        "outputId": "789864e1-5a42-4df6-a7cd-4cea1102cde2"
      },
      "outputs": [],
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "#from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a2bac93"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('hw5_data.csv')\n",
        "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb34060d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = data['SeriousDlqin2yrs']\n",
        "X = data.drop(['SeriousDlqin2yrs'],axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)\n",
        "\n",
        "print('train:',X_train.shape, y_train.shape)\n",
        "print('test:',X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90d16082"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
        "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
        "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from time import time\n",
        "import xgboost\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "420aeb16"
      },
      "outputs": [],
      "source": [
        "columns_list = list(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f40f9107"
      },
      "source": [
        "a. (2.5 pts) Fit a Decision Tree Classifier with random_state = 14 for this classification problem. Report the accuracy_score and roc_auc_score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45de59e6"
      },
      "outputs": [],
      "source": [
        "def fit_classifier(clf):\n",
        "  # Fit the classifier on the training set\n",
        "  ### START CODE ###\n",
        "  ### END CODE ###\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZfiYRKtHJQ4"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(clf, X_test, y_test):\n",
        "  # Compute the accuracy_score, and roc_auc_score on the test set\n",
        "  ### START CODE ###\n",
        "  y_pred = None\n",
        "  y_pred_proba = None\n",
        "\n",
        "  acc_score = accuracy_score(None, None)\n",
        "  auc_score = roc_auc_score(None, None)\n",
        "  ### END CODE ###\n",
        "  print(\"Accuracy_score: {}, ROC_AUC_score: {}\".format(acc_score, auc_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a436542"
      },
      "outputs": [],
      "source": [
        "print(\"Decision Tree\")\n",
        "# Initialize your decision tree classifier\n",
        "### START CODE ###\n",
        "dt_clf = None\n",
        "### END CODE ###\n",
        "\n",
        "dt_clf = fit_classifier(dt_clf)\n",
        "evaluate_classifier(dt_clf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d360a99"
      },
      "source": [
        "b. (2.5 pts) Create a [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) of 25 classifiers (i.e, n_estimators=25) with random_state=14. Please use Decision Tree Classifier with random_state=14 as the base classifier. Report accuracy_score and roc_auc_score on the test data for this emsemble classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "239c9c3c"
      },
      "outputs": [],
      "source": [
        "print(\"Bagging of Decesion Trees\")\n",
        "# Initialize your bagging classifier\n",
        "### START CODE ###\n",
        "bag_clf = None\n",
        "### END CODE ###\n",
        "\n",
        "bag_clf = fit_classifier(bag_clf)\n",
        "evaluate_classifier(bag_clf, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34225ee3"
      },
      "source": [
        "c. (5 pts) In this question, you will fit a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on the training data for this classification task.\n",
        "\n",
        "1. First, please find the best parameters (including *n_estimators*, *max_features* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the optimal parameters obtained by GridSearch.\n",
        "2. Fit a model using the best parameters, and report the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9E7sMGt2goM"
      },
      "outputs": [],
      "source": [
        "def grid_search_for_classifier(clf, param_grid, X_train, y_train):\n",
        "  # Grid search\n",
        "  grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
        "\n",
        "  # Conduct grid search using the training set (1 line of code only)\n",
        "  ### START CODE ###\n",
        "  ### END CODE ###\n",
        "  print(grid_search.best_params_)\n",
        "\n",
        "  # Set the best paramters for your clf (1 line of code only)\n",
        "  ### START CODE ###\n",
        "  ### END CODE ###\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UyA2_rB2gG9"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
        "  t0 = time()\n",
        "  # Fit your classifier on the training set\n",
        "  ### START CODE ###\n",
        "  ### END CODE ###\n",
        "  print(\"training time\", round(time()-t0, 3), \"s\")\n",
        "\n",
        "  t0 = time()\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(\"predict time\", round(time()-t0, 3), \"s\")\n",
        "\n",
        "  print(\"Confusion matrix: \")\n",
        "  # Print the confusion matrix computed from the test set (1 line of code only)\n",
        "  ### START CODE ###\n",
        "  ### END CODE ###\n",
        "\n",
        "\n",
        "  ### START CODE ###\n",
        "  y_pred_proba = None\n",
        "  acc_score = accuracy_score(None, None)\n",
        "  auc_score = roc_auc_score(None, None)\n",
        "  ### END CODE ###\n",
        "\n",
        "  print(\"Accuracy: {}, AUC_ROC: {}\".format(acc_score, auc_score))\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bfb9542"
      },
      "outputs": [],
      "source": [
        "param_grid = {\"n_estimators\": [1, 10, 50, 100],\n",
        "              \"max_features\": [1, 5, 10, \"auto\"],\n",
        "              \"criterion\": ['gini','entropy'],\n",
        "              \"random_state\": [17]}\n",
        "\n",
        "# Initialize your random forest classifier\n",
        "### START CODE ###\n",
        "rf_clf = None\n",
        "### END CODE ###\n",
        "rf_clf = grid_search_for_classifier(rf_clf, param_grid, X_train, y_train)\n",
        "train_and_evaluate_classifier(rf_clf, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "314c313b"
      },
      "source": [
        "d. (10 pts) This time, let us use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) for the same task. For AdaBoost and XGBoost, please respectively find the best parameters (including *n_estimators, learning_rate*); fit your model using the best parameters, and report the confusion matrix and roc_auc_score on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3222ef25"
      },
      "outputs": [],
      "source": [
        "param_grid = {\"n_estimators\": [10, 100],\n",
        "          \"learning_rate\": [0.01, 0.1, 0.5],\n",
        "          \"random_state\": [17]\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp4Ekjpy1cGw"
      },
      "outputs": [],
      "source": [
        "# Initialize your AdaBoost classifier\n",
        "### START CODE ###\n",
        "ab_clf = None\n",
        "### END CODE ###\n",
        "ab_clf = grid_search_for_classifier(ab_clf, param_grid, X_train, y_train)\n",
        "train_and_evaluate_classifier(ab_clf, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv0p1mZh2CqA"
      },
      "outputs": [],
      "source": [
        "# Initialize your XGBoost classifier\n",
        "### START CODE ###\n",
        "xgb_clf = None\n",
        "### END CODE ###\n",
        "xgb_clf = grid_search_for_classifier(xgb_clf, param_grid, X_train, y_train)\n",
        "train_and_evaluate_classifier(xgb_clf, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6bc87cd"
      },
      "source": [
        "f. (5 pts) Compare the performance of decision tree from part a) with the ensemble methods. Briefly explain which of the three ensemble methods performed better and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZXTR1tM7yBp"
      },
      "source": [
        "# Q3: CatBoost (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zcExque_yso"
      },
      "source": [
        "In this question you will learn about a boosting algorithm known as **CatBoost**. Please go through the two videos specified below to get a better understanding of the CatBoost algorithm and answer the questions that follow.\n",
        "\n",
        "[Part-1](https://www.youtube.com/watch?v=KXOTSkPL2X4&ab_channel=StatQuestwithJoshStarmer)\n",
        "[Part - 2](https://www.youtube.com/watch?v=3Bg2XRFOTzg&t=242s&ab_channel=StatQuestwithJoshStarmer)\n",
        "\n",
        "\n",
        "\n",
        "a. **(5 points)** Briefly explain Ordered Target Encoding. What challenge does it try to address?\n",
        "\n",
        "b. **(5 points)** Briefly describe the main advantages and disadvantages of CatBoost as compared to XGBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFRT23o76A2"
      },
      "source": [
        "# Q4: Convolutional Neural Network (20 points)\n",
        "In this question, we will continue our exercise on the SVHN classification task from the previous homework, but this time we will be using Convolutional Neural Networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW0odgzmXUU_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr0KdJ2LrCCV"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBCdEj-_0tkS"
      },
      "outputs": [],
      "source": [
        "transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n",
        "        ])\n",
        "\n",
        "train_dataset = torchvision.datasets.SVHN(root='.', split='train', transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.SVHN(root='.', split='test', transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLn4YKK90vjo"
      },
      "outputs": [],
      "source": [
        "train_num = int(len(train_dataset) * 0.8)\n",
        "val_num = len(train_dataset) - train_num\n",
        "# Randomly split the training dataset into training dataset and validation dataset\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_num, val_num])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUXsbsqjoCw"
      },
      "source": [
        "a. (10 points) Build a convolutional neural network with the following sequential configuration. If not specified, please use the default setting of torch.nn.Conv2d. The output of the convolution layers will be fed into a fully-connected MLP. Then train the model with Adam optimizer (lr=1e-3) for 10 epochs. You should be able to achieve test accuracy of over 85%.\n",
        "\n",
        "\n",
        "\n",
        "> Layer 1\n",
        "*   2d convolution (# input channel=3, # output channel=16, kernel size=3, padding=1)\n",
        "*   2d batch normalization\n",
        "*   Relu activation\n",
        "\n",
        "> Pool 1\n",
        "*   2d max pooling (kernel size=2)\n",
        "\n",
        "> Layer 2\n",
        "*   2d convolution (# output channel=16, kernel size=3, padding=1)\n",
        "*   2d batch normalization\n",
        "*   Relu activation\n",
        "\n",
        "> Pool 2\n",
        "*   2d max pooling (kernel size=2)\n",
        "\n",
        "> Layer 3\n",
        "*   2d convolution (# output channel=32, kernel size=3, padding=1)\n",
        "*   2d batch normalization\n",
        "*   Relu activation\n",
        "\n",
        "> Pool 3\n",
        "*   2d max pooling (kernel size=2)\n",
        "\n",
        "References:\n",
        "\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
        "*   https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8OcO88V01Rr"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, pool=True):\n",
        "        super(CNN, self).__init__()\n",
        "        self.pool = pool\n",
        "\n",
        "        # Create convolutional layers\n",
        "        ### START CODE ###\n",
        "        self.layer1 = nn.Sequential()\n",
        "\n",
        "        self.pool1 = None\n",
        "\n",
        "        self.layer2 = nn.Sequential()\n",
        "\n",
        "        self.pool2 = None\n",
        "\n",
        "        self.layer3 = nn.Sequential()\n",
        "\n",
        "        self.pool3 = None\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Create fully connected layers (nn.Linear)\n",
        "        if self.pool:\n",
        "            self.mlp1 = nn.Linear(32*4*4, 50)\n",
        "        else:\n",
        "            self.mlp1 = nn.Linear(32*32*32, 50)\n",
        "\n",
        "        self.mlp2 = nn.Linear(50, 50)\n",
        "        self.mlp3 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        if self.pool:\n",
        "            x = self.pool1(x)\n",
        "\n",
        "        x = self.layer2(x)\n",
        "        if self.pool:\n",
        "            x = self.pool2(x)\n",
        "\n",
        "        x = self.layer3(x)\n",
        "        if self.pool:\n",
        "            x = self.pool3(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        x = F.relu(self.mlp1(x))\n",
        "        x = F.relu(self.mlp2(x))\n",
        "        x = self.mlp3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygrxekov097w"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_num = 0\n",
        "    for data, target in tqdm(loader):\n",
        "        out = model(data)\n",
        "        # Calculate loss based on model output and target\n",
        "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
        "\n",
        "        # Use the optimizer to perform backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = len(target)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_num += batch_size\n",
        "    avg_loss = total_loss / total_num\n",
        "    return avg_loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_num = 0\n",
        "    for data, target in tqdm(loader):\n",
        "        out = model(data)\n",
        "        # Calculate loss based on model output and target\n",
        "        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n",
        "\n",
        "        # Get model's prediction\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "\n",
        "        # Count number of correct predictions\n",
        "        correct = accuracy_score(target, pred, normalize=False)\n",
        "\n",
        "        total_correct += correct\n",
        "        batch_size = len(target)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_num += batch_size\n",
        "    avg_loss = total_loss / total_num\n",
        "    acc = total_correct / total_num\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aET_5IqXDsC1"
      },
      "outputs": [],
      "source": [
        "model1 = CNN(pool=True)\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
        "best_acc = -np.inf\n",
        "epochs = 10\n",
        "for e in range(1, epochs + 1):\n",
        "    train_loss = train(model1, train_loader, optimizer)\n",
        "    val_loss, val_acc = eval(model1, val_loader)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model1 = model1\n",
        "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Za_DdlDwJX"
      },
      "outputs": [],
      "source": [
        "_, test_acc = eval(best_model1, test_loader)\n",
        "print(f\"Test accuracy: {np.round(test_acc, 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLAnNsjsnRpK"
      },
      "source": [
        "b. (5 points) Use torch-summary to print a summary of the model. The number of parameters should be less than the one of the MLP we trained in the previous homework. Why does it have less number of parameters but have higher accuracy?\n",
        "\n",
        "Reference\n",
        "*   https://pypi.org/project/torch-summary/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkuRsoNxR8dX"
      },
      "outputs": [],
      "source": [
        "!pip install torch-summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eb6rNHsobVb"
      },
      "source": [
        "c. (5 points) Train another CNN with the pool option set to False. What are the differences in terms of accuracy or computation caused by disabling max pooling? What are the effects of pooling operations in CNNs? (This might take some time. Watch a TV show while you're waiting for the results..)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m-9yG9VAbgE"
      },
      "outputs": [],
      "source": [
        "model2 = CNN(pool=False)\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "best_acc = -np.inf\n",
        "epochs = 10\n",
        "for e in range(1, epochs + 1):\n",
        "    train_loss = train(model2, train_loader, optimizer)\n",
        "    val_loss, val_acc = eval(model2, val_loader)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        best_model2 = model2\n",
        "    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13gIhnYVAbgQ"
      },
      "outputs": [],
      "source": [
        "_, test_acc = eval(best_model2, test_loader)\n",
        "print(test_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
